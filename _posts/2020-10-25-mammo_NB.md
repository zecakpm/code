---
title: "Breast Cancer Classification with Naive Bayes"
date: 2020-10-25
tags: [machine learning, classification, Naive Bayes, data science]
header:
  image: "/images/naive_bayes/naive_bayes_.jpg"
excerpt: "Data Cleaning, Machine Learning, Data Science"
---



## Naive Bayes Classifier

For this post I will explore Naive Bayes classifier to predict Breast Cancer.
Naive Bayes is a supervised learning algorithm. It has the main premise that each variable will impact the result in an independent manner. 

## Understanding the data
Find the data for download here --> [Mammographic Mass Data Set](https://archive.ics.uci.edu/ml/datasets/Mammographic+Mass)

# Columns

1. BI-RADS assessment: 1 to 5 (ordinal)
2. Age: patient's age in years (integer)
3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)
4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 5. spiculated=5 (nominal)
6. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)
7. Severity: benign=0 or malignant=1 (binominal)

BI-RADS will be discarded as is an assessment of serivity, predict variable of our project.

## Loading libraries and data
First select all libraries.\
Also at this stage renamed the columns using the data documentation.


```python
import pandas as pd
import numpy as np
import urllib
import sklearn
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import train_test_split

from sklearn import metrics
from sklearn.metrics import accuracy_score


input_file ='mammographic_masses.data.txt'
df = pd.read_csv(input_file)
df.columns = ['BI-RADS','age','shape','margin','density','severity']

```

## Visualising, cleaning, and transforming the data

Evaluating the data set is easy to spot some missing values presented by "?" sign.\
The evaluation has to be made at this point for outliers, the function min_max will select min and max values for each column, then we will be able to compare it with the values on the documentation.

<img src="{{ site.url }}{{ site.baseurl }}/images/naive_bayes/1.jpg" alt="linearly separable data">

```python
#remove missing values
df = df.replace('?',np.nan)
df = df.dropna()

#checking for outliers
def min_max (df):
    min_col = {}
    max_col = {}
    
    for col in df:
        max_col[col] = df[col].max()
        min_col[col] = df[col].min()
        
    result = pd.DataFrame([min_col,max_col])
    return result
min_max(df)

```
<img src="{{ site.url }}{{ site.baseurl }}/images/naive_bayes/2.jpg" alt="linearly separable data">

Also lets check if all the columns are integers, this will turn out to be essencial to run the NB classifier later on.

```python
#checking types
df.dtypes

#tranforming all columns to integer
df= df.astype(str).astype(int) 

```
## Before
<img src="{{ site.url }}{{ site.baseurl }}/images/naive_bayes/3.jpg" alt="linearly separable data">
## After
<img src="{{ site.url }}{{ site.baseurl }}/images/naive_bayes/4.jpg" alt="linearly separable data">



## Selecting and Splitting the data
The severity column its the aim of our prediction (target), so lets remove it from the others. On the other side will be the remaining columns (features -->  (age,shape,margin,density).

```python
features_col = ['age', 'shape', 'margin', 'density']
features = df[features_col]
target = df[['severity']]

#splitting the data
#test ==> 20%
#random state ==> 17, random number to control the suffle applied to the data before spliting
features_train, features_test, target_train, target_test = train_test_split(features,target,test_size = 0.2, random_state =17)
```

## Validating classifiers

This section will be divided into 2 parts, first using Bernoulli Naive Bayes,and second Multinominal Naive Bayes.
* Train both models with 80% of the data
* Test the models with 20% of the data
* Compare the accuracy values of each of the model variants 

```python
#Multinominal Naive Bayes
MultiNB = MultinomialNB()
MultiNB.fit(features_train,target_train)

target_expect = target_test
target_pred = MultiNB.predict(features_test)
print(accuracy_score(target_expect,target_pred))


#Bernoulli Naive Bayes
BernNB = BernoulliNB(binarize = True)
BernNB.fit(features_train,target_train)
print(BernNB)

target_expect = target_test
target_pred = BernNB.predict(features_test)
print(accuracy_score(target_expect,target_pred))

```

Our results show Multinominal Naive Bayes with an accuracy of 0.75 and Bernoulli Naive Bayes with 0.737.\
Results could refined/tuned to increase the accuracy, also other classifiers could be contrasted with the results found.
